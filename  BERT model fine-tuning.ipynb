{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1db35c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler\n",
    "from transformers import AutoTokenizer, BertModel, get_linear_schedule_with_warmup, utils, logging\n",
    "import time\n",
    "from tqdm import trange \n",
    "from bertviz import head_view\n",
    "from statistics import mean \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5522f19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load task 1 positive and negative datasets\n",
    "\n",
    "df_positive_train = pd.read_csv('data/df_positive_train.csv', index_col=0)\n",
    "df_negative_train = pd.read_csv('data/df_negative_1_train.csv', index_col=0)\n",
    "\n",
    "df_positive_test = pd.read_csv('data/df_positive_train.csv', index_col=0)\n",
    "df_negative_test = pd.read_csv('data/df_negative_1_train.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d651988a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% prep data for modelling\n",
    "\n",
    "df_positive_train['y'] = 1\n",
    "df_negative_train['y'] = 0\n",
    "\n",
    "df_train = pd.concat([df_positive_train, df_negative_train])\n",
    "\n",
    "df_positive_test['y'] = 1\n",
    "df_negative_test['y'] = 0\n",
    "\n",
    "df_test = pd.concat([df_positive_test, df_negative_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f998604d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['triple'] = df_train.head_name + ' ' + df_train.link + ' ' + df_train.tail_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8071e78b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>head_id</th>\n",
       "      <th>head_name</th>\n",
       "      <th>link</th>\n",
       "      <th>tail_id</th>\n",
       "      <th>tail_name</th>\n",
       "      <th>label</th>\n",
       "      <th>y</th>\n",
       "      <th>triple</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CHEBI:111499</td>\n",
       "      <td>N-[(2S,3R)-2-[[1,3-benzodioxol-5-ylmethyl(meth...</td>\n",
       "      <td>is_a</td>\n",
       "      <td>CHEBI:36963</td>\n",
       "      <td>organooxygen compound</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>N-[(2S,3R)-2-[[1,3-benzodioxol-5-ylmethyl(meth...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CHEBI:34985</td>\n",
       "      <td>Stylisterol C</td>\n",
       "      <td>is_a</td>\n",
       "      <td>CHEBI:188017</td>\n",
       "      <td>5beta-Cholane-3alpha,6alpha,24-triol</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Stylisterol C is_a 5beta-Cholane-3alpha,6alpha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CHEBI:131773</td>\n",
       "      <td>tricin 4'-O-(erythro-beta-guaiacylglyceryl) et...</td>\n",
       "      <td>is_a</td>\n",
       "      <td>CHEBI:23798</td>\n",
       "      <td>dimethoxyflavone</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>tricin 4'-O-(erythro-beta-guaiacylglyceryl) et...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CHEBI:95766</td>\n",
       "      <td>(8R,9S)-6-[(2R)-1-hydroxypropan-2-yl]-8-methyl...</td>\n",
       "      <td>is_a</td>\n",
       "      <td>CHEBI:52898</td>\n",
       "      <td>azamacrocycle</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>(8R,9S)-6-[(2R)-1-hydroxypropan-2-yl]-8-methyl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CHEBI:50366</td>\n",
       "      <td>6-methylprednisolone</td>\n",
       "      <td>is_a</td>\n",
       "      <td>CHEBI:35346</td>\n",
       "      <td>11beta-hydroxy steroid</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6-methylprednisolone is_a 11beta-hydroxy steroid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>CHEBI:104299</td>\n",
       "      <td>N-[(4S,7S,8R)-8-methoxy-4,5,7,10-tetramethyl-1...</td>\n",
       "      <td>is_a</td>\n",
       "      <td>CHEBI:24995</td>\n",
       "      <td>lactam</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>N-[(4S,7S,8R)-8-methoxy-4,5,7,10-tetramethyl-1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>CHEBI:68354</td>\n",
       "      <td>poricoic acid C</td>\n",
       "      <td>is_a</td>\n",
       "      <td>CHEBI:35692</td>\n",
       "      <td>dicarboxylic acid</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>poricoic acid C is_a dicarboxylic acid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>CHEBI:6402</td>\n",
       "      <td>leflunomide</td>\n",
       "      <td>has_role</td>\n",
       "      <td>CHEBI:35475</td>\n",
       "      <td>non-steroidal anti-inflammatory drug</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>leflunomide has_role non-steroidal anti-inflam...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>CHEBI:59771</td>\n",
       "      <td>8-chlorotheophylline</td>\n",
       "      <td>is_a</td>\n",
       "      <td>CHEBI:166894</td>\n",
       "      <td>xanthonolignoid</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8-chlorotheophylline is_a xanthonolignoid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>CHEBI:50566</td>\n",
       "      <td>nitric oxide donor</td>\n",
       "      <td>is_a</td>\n",
       "      <td>CHEBI:17891</td>\n",
       "      <td>donor</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>nitric oxide donor is_a donor</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>250 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          head_id                                          head_name  \\\n",
       "0    CHEBI:111499  N-[(2S,3R)-2-[[1,3-benzodioxol-5-ylmethyl(meth...   \n",
       "1     CHEBI:34985                                      Stylisterol C   \n",
       "2    CHEBI:131773  tricin 4'-O-(erythro-beta-guaiacylglyceryl) et...   \n",
       "3     CHEBI:95766  (8R,9S)-6-[(2R)-1-hydroxypropan-2-yl]-8-methyl...   \n",
       "4     CHEBI:50366                               6-methylprednisolone   \n",
       "..            ...                                                ...   \n",
       "245  CHEBI:104299  N-[(4S,7S,8R)-8-methoxy-4,5,7,10-tetramethyl-1...   \n",
       "246   CHEBI:68354                                    poricoic acid C   \n",
       "247    CHEBI:6402                                        leflunomide   \n",
       "248   CHEBI:59771                               8-chlorotheophylline   \n",
       "249   CHEBI:50566                                 nitric oxide donor   \n",
       "\n",
       "         link       tail_id                             tail_name  label  y  \\\n",
       "0        is_a   CHEBI:36963                 organooxygen compound      1  1   \n",
       "1        is_a  CHEBI:188017  5beta-Cholane-3alpha,6alpha,24-triol      0  0   \n",
       "2        is_a   CHEBI:23798                      dimethoxyflavone      1  1   \n",
       "3        is_a   CHEBI:52898                         azamacrocycle      1  1   \n",
       "4        is_a   CHEBI:35346                11beta-hydroxy steroid      1  1   \n",
       "..        ...           ...                                   ...    ... ..   \n",
       "245      is_a   CHEBI:24995                                lactam      1  1   \n",
       "246      is_a   CHEBI:35692                     dicarboxylic acid      1  1   \n",
       "247  has_role   CHEBI:35475  non-steroidal anti-inflammatory drug      2  1   \n",
       "248      is_a  CHEBI:166894                       xanthonolignoid      0  0   \n",
       "249      is_a   CHEBI:17891                                 donor      1  1   \n",
       "\n",
       "                                                triple  \n",
       "0    N-[(2S,3R)-2-[[1,3-benzodioxol-5-ylmethyl(meth...  \n",
       "1    Stylisterol C is_a 5beta-Cholane-3alpha,6alpha...  \n",
       "2    tricin 4'-O-(erythro-beta-guaiacylglyceryl) et...  \n",
       "3    (8R,9S)-6-[(2R)-1-hydroxypropan-2-yl]-8-methyl...  \n",
       "4     6-methylprednisolone is_a 11beta-hydroxy steroid  \n",
       "..                                                 ...  \n",
       "245  N-[(4S,7S,8R)-8-methoxy-4,5,7,10-tetramethyl-1...  \n",
       "246             poricoic acid C is_a dicarboxylic acid  \n",
       "247  leflunomide has_role non-steroidal anti-inflam...  \n",
       "248          8-chlorotheophylline is_a xanthonolignoid  \n",
       "249                      nitric oxide donor is_a donor  \n",
       "\n",
       "[250 rows x 8 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_subset = df_train.sample(500, random_state=101)\n",
    "\n",
    "df_train_subset = df_subset.iloc[:250]\n",
    "df_val_subset = df_subset.iloc[250:]\n",
    "\n",
    "df_train_subset.reset_index(inplace=True, drop=True)\n",
    "df_val_subset.reset_index(inplace=True, drop=True)\n",
    "\n",
    "df_val_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e493b924",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define convenience functions to: (i) calculate scoring metrics accuracy, precision recall, specificity; \n",
    "# (ii) calculate numbers of correctly classified instances\n",
    "\n",
    "# Convenience functions to calculate scoring metrics accuracy, precision recall, specificity\n",
    "def b_tp(preds, labels):\n",
    "    \"\"\"\n",
    "    Returns True Positives (TP): count of correct predictions of actual class 1\n",
    "    \"\"\"\n",
    "    return sum([preds == labels and preds == 1 for preds, labels in zip(preds, labels)])\n",
    "\n",
    "def b_fp(preds, labels):\n",
    "    \"\"\" \n",
    "    Returns False Positives (FP): count of wrong predictions of actual class 1\n",
    "    \"\"\"\n",
    "    return sum([preds != labels and preds == 1 for preds, labels in zip(preds, labels)])\n",
    "\n",
    "def b_tn(preds, labels):\n",
    "    \"\"\"\n",
    "    Returns True Negatives (TN): count of correct predictions of actual class 0\n",
    "    \"\"\"\n",
    "    return sum([preds == labels and preds == 0 for preds, labels in zip(preds, labels)])\n",
    "\n",
    "def b_fn(preds, labels):\n",
    "    \"\"\"\n",
    "    Returns False Negatives (FN): count of wrong predictions of actual class 0\n",
    "    \"\"\"\n",
    "    return sum([preds != labels and preds == 0 for preds, labels in zip(preds, labels)])\n",
    "\n",
    "def b_fn(preds, labels):\n",
    "    \"\"\"\n",
    "    Returns False Negatives (FN): count of wrong predictions of actual class 0\n",
    "    \"\"\"\n",
    "    return sum([preds != labels and preds == 0 for preds, labels in zip(preds, labels)])\n",
    "\n",
    "def b_metrics(preds, labels):\n",
    "    \"\"\"\n",
    "    Returns (i) accuracy: (TP + TN) / N; (ii) precision: TP / (TP + FP); (iii) recall: TP / (TP + FN);\n",
    "    (iv) specificity: TN / (TN + FP)\n",
    "    \"\"\"\n",
    "    tp = b_tp(preds, labels)\n",
    "    tn = b_tn(preds, labels)\n",
    "    fp = b_fp(preds, labels)\n",
    "    fn = b_fn(preds, labels)\n",
    "    b_accuracy = (tp + tn) / len(labels)\n",
    "    b_precision = tp / (tp + fp) if (tp + fp) > 0 else 'nan'\n",
    "    b_recall = tp / (tp + fn) if (tp + fn) > 0 else 'nan'\n",
    "    b_specificity = tn / (tn + fp) if (tn + fp) > 0 else 'nan'\n",
    "    return b_accuracy, b_precision, b_recall, b_specificity\n",
    "\n",
    "# Convenience function to calculate numbers of correctly classified instances\n",
    "def calculate_accuracy(preds, labels):\n",
    "    n_correct = (preds==labels).sum().item()\n",
    "    return n_correct\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e8f8cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed\n",
    "\n",
    "seed = 42\n",
    "\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4f33ba82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dataset class\n",
    "\n",
    "class TriplesData(Dataset):\n",
    "    \n",
    "    def __init__(self, tokenizer, dataframe, max_len=512):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.text = dataframe['triple']\n",
    "        self.targets = dataframe['y']\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text = str(self.text[index])\n",
    "        text = \" \".join(text.split())\n",
    "        inputs = self.tokenizer.encode_plus(text, None, add_special_tokens=True, max_length=self.max_len,\n",
    "                                            pad_to_max_length=True, return_token_type_ids=True)\n",
    "        ids = inputs['input_ids']\n",
    "        mask = inputs['attention_mask']\n",
    "        token_type_ids = inputs[\"token_type_ids\"]\n",
    "\n",
    "        return {\n",
    "            'ids': torch.tensor(ids, dtype=torch.long),\n",
    "            'mask': torch.tensor(mask, dtype=torch.long),\n",
    "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
    "            'targets': torch.tensor(self.targets[index], dtype=torch.float)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9dabb344",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dataset object for training subset\n",
    "\n",
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract\")\n",
    "\n",
    "# Create Dataset objects\n",
    "training_set = TriplesData(tokenizer, df_train_subset)\n",
    "val_set = TriplesData(tokenizer, df_val_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2e43f260",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Bert model class for fine-tuning for binary classification. Model incorporates two linear layers \n",
    "# on top of base Bert model with RELU as activation function\n",
    "\n",
    "class BertClass(torch.nn.Module):\n",
    "    def __init__(self, model, dropout):\n",
    "        super(BertClass, self).__init__()\n",
    "        self.model = model\n",
    "        self.l1 = BertModel.from_pretrained(self.model, num_labels=2, output_attentions=True)\n",
    "        self.pre_classifier = torch.nn.Linear(768, 768)\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "        self.classifier = torch.nn.Linear(768, 2)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "\n",
    "    # Define forward\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        output_1 = self.l1(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        hidden_state = output_1[0]\n",
    "        pooler = hidden_state[:, 0]\n",
    "        pooler = self.pre_classifier(pooler)\n",
    "        pooler = self.relu(pooler)\n",
    "        pooler = self.dropout(pooler)\n",
    "        output = self.classifier(pooler)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "06897d21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertClass(\n",
       "  (l1): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       "  (relu): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialise PubMedBERT model, freeze first 8 layers, send to device\n",
    "\n",
    "model = BertClass(model='microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract', dropout=0.1)\n",
    "\n",
    "for name, param in list(model.named_parameters())[0:133]: \n",
    "    param.requires_grad = False\n",
    "    \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7c0eeb95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function to run fine-tuning training and validation and print metrics \n",
    "\n",
    "def run_finetuning(model_id, epochs, training_loader, testing_loader, loss_function, optimizer, warmup=True):\n",
    "    \n",
    "    logging.set_verbosity_error()\n",
    "    if warmup:\n",
    "        total_steps = len(training_loader) * epochs\n",
    "        scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "    \n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "    val_losses = []\n",
    "    val_accuracies = []\n",
    "    \n",
    "    for epoch in trange(epochs, desc = 'Epoch'):\n",
    "        logging.set_verbosity_error()\n",
    "        \n",
    "        start = time.time()\n",
    "        \n",
    "        total_train_loss = 0\n",
    "        num_tr_correct = 0\n",
    "        num_tr_steps = 0\n",
    "        num_tr_examples = 0\n",
    "    \n",
    "        model_id.train()\n",
    "        \n",
    "        print('-'*20)\n",
    "        print(\"Training...\")\n",
    "    \n",
    "        for step, batch in enumerate(training_loader):\n",
    "\n",
    "            train_ids = batch['ids'].to(device, dtype = torch.long)\n",
    "            train_masks = batch['mask'].to(device, dtype = torch.long)\n",
    "            train_token_type_ids = batch['token_type_ids'].to(device, dtype = torch.long)\n",
    "            train_targets = batch['targets'].to(device, dtype = torch.long)\n",
    "        \n",
    "            model_id.zero_grad() \n",
    "        \n",
    "            train_outputs = model_id(train_ids, train_masks, train_token_type_ids)\n",
    "            loss = loss_function(train_outputs, train_targets)\n",
    "            big_val, big_idx = torch.max(train_outputs.data, dim=1)\n",
    "            num_tr_correct += calculate_accuracy(big_idx, train_targets)\n",
    "        \n",
    "            total_train_loss += loss.item()\n",
    "            num_tr_steps += 1\n",
    "            num_tr_examples += train_targets.size(0)\n",
    "            \n",
    "            #print('-'*20)\n",
    "            #print('Tracking')\n",
    "            #print(f\"Loss: {total_train_loss / num_tr_steps}\")\n",
    "            #print(f\"Accuracy: {num_tr_correct/num_tr_examples}\")\n",
    "            #print('-'*20)\n",
    "        \n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model_id.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            if warmup:\n",
    "                scheduler.step()\n",
    "        \n",
    "        average_train_loss = total_train_loss / num_tr_steps\n",
    "        train_losses.append(average_train_loss)\n",
    "        train_accuracy = num_tr_correct/num_tr_examples\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        \n",
    "        print('-'*20)\n",
    "        print(f\"Epoch {epoch}:\")\n",
    "        print(f\"Average training loss: {average_train_loss}\")\n",
    "        print(f\"Training accuracy: {train_accuracy}\")\n",
    "              \n",
    "        model_id.eval()\n",
    "        print(\"\\n\\nEvaluation...\")\n",
    "        \n",
    "        # Tracking variables \n",
    "        total_val_loss = 0\n",
    "        num_val_correct = 0\n",
    "        num_val_steps = 0\n",
    "        num_val_examples = 0\n",
    "        \n",
    "        val_accuracy = []\n",
    "        val_precision = []\n",
    "        val_recall = []\n",
    "        val_specificity = []\n",
    "        \n",
    "        for step, batch in enumerate(testing_loader, 0):\n",
    "\n",
    "            val_ids = batch['ids'].to(device, dtype = torch.long)\n",
    "            val_masks = batch['mask'].to(device, dtype = torch.long)\n",
    "            val_token_type_ids = batch['token_type_ids'].to(device, dtype = torch.long)\n",
    "            val_targets = batch['targets'].to(device, dtype = torch.long)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = model_id(val_ids, val_masks, val_token_type_ids)\n",
    "                loss = loss_function(outputs, val_targets)\n",
    "                big_val, big_idx = torch.max(outputs.data, dim=1)\n",
    "                num_val_correct += calculate_accuracy(big_idx, val_targets)\n",
    "                \n",
    "                total_val_loss += loss.item()\n",
    "                num_val_steps += 1\n",
    "                num_val_examples += train_targets.size(0)\n",
    "            \n",
    "            # Calculate validation metrics\n",
    "            b_accuracy, b_precision, b_recall, b_specificity = b_metrics(big_idx, val_targets)\n",
    "            val_accuracy.append(b_accuracy)\n",
    "            # Update precision only when (tp + fp) !=0; ignore nan\n",
    "            if b_precision != 'nan': val_precision.append(b_precision)\n",
    "            # Update recall only when (tp + fn) !=0; ignore nan\n",
    "            if b_recall != 'nan': val_recall.append(b_recall)\n",
    "            # Update specificity only when (tn + fp) !=0; ignore nan\n",
    "            if b_specificity != 'nan': val_specificity.append(b_specificity)\n",
    "                \n",
    "        end = time.time() \n",
    "        \n",
    "        average_val_loss = total_val_loss / num_val_steps\n",
    "        val_losses.append(average_val_loss)\n",
    "        batch_val_acc = sum(val_accuracy)/len(val_accuracy)\n",
    "        val_accuracies.append(batch_val_acc)\n",
    "        \n",
    "        print('-'*20)\n",
    "        print(f\"Average validation loss: {average_val_loss}\")\n",
    "        print(f\"Validation Accuracy: {batch_val_acc:.4f}\")\n",
    "        print('Validation Precision: {:.4f}'.format(sum(val_precision)/len(val_precision)) if \n",
    "              len(val_precision)>0 else 'Validation Precision: NaN')\n",
    "        print('Validation Recall: {:.4f}'.format(sum(val_recall)/len(val_recall)) if \n",
    "              len(val_recall)>0 else 'Validation Recall: NaN')\n",
    "        print('Validation Specificity: {:.4f}\\n'.format(sum(val_specificity)/len(val_specificity)) if \n",
    "              len(val_specificity)>0 else 'Validation Specificity: NaN')\n",
    "        print(f\"Time elapsed: {(start-end)/60:.0f}mins\\n\")\n",
    "     \n",
    "    # Plot the learning curve.\n",
    "    df_stats = pd.DataFrame({'Epochs': range(0, epochs), 'Train Loss': train_losses, \n",
    "                      'Train accuracy': train_accuracies, 'Validation Loss': val_losses, \n",
    "                      'Validation accuracy': val_accuracies})\n",
    "    sns.set(style='darkgrid')\n",
    "    plt.rcParams[\"figure.figsize\"] = (12,6)\n",
    "    plt.plot(df_stats['Train Loss'], 'b-o', label=\"Training loss\")\n",
    "    plt.plot(df_stats['Validation Loss'], 'g-o', label=\"Validation loss\")\n",
    "    plt.plot(df_stats['Train accuracy'], 'r-o', label=\"Training accuracy\")\n",
    "    plt.plot(df_stats['Validation accuracy'], 'y-o', label=\"Validation accuracy\")\n",
    "    plt.title(\"Training & Validation Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy/Loss\")\n",
    "    plt.legend()\n",
    "    plt.xticks([1, 2, 3, 4])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b87fc0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoader objects to pass training and validation data to model using RandomSampler for reproducibility\n",
    "\n",
    "train_sampler = RandomSampler(data_source=training_set)\n",
    "val_sampler = RandomSampler(data_source=val_set)\n",
    "\n",
    "training_loader = DataLoader(training_set, batch_size=16, sampler=train_sampler, num_workers=0)\n",
    "val_loader = DataLoader(val_set, batch_size=16, sampler=val_sampler, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1e5048e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   0%|                                              | 0/7 [00:00<?, ?it/s]/Users/emilygroves/opt/anaconda3/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2339: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "Training...\n",
      "--------------------\n",
      "Epoch 0:\n",
      "Average training loss: 0.4381577419117093\n",
      "Training accuracy: 0.816\n",
      "\n",
      "\n",
      "Evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  14%|████▊                             | 1/7 [19:45<1:58:32, 1185.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "Average validation loss: 0.39308224618434906\n",
      "Validation Accuracy: 0.8531\n",
      "Validation Precision: 0.8105\n",
      "Validation Recall: 0.9658\n",
      "Validation Specificity: 0.7221\n",
      "\n",
      "Time elapsed: -20mins\n",
      "\n",
      "--------------------\n",
      "Training...\n",
      "--------------------\n",
      "Epoch 1:\n",
      "Average training loss: 0.3167569041252136\n",
      "Training accuracy: 0.888\n",
      "\n",
      "\n",
      "Evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  29%|█████████▋                        | 2/7 [40:01<1:40:16, 1203.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "Average validation loss: 0.30825814697891474\n",
      "Validation Accuracy: 0.8742\n",
      "Validation Precision: 0.8593\n",
      "Validation Recall: 0.9003\n",
      "Validation Specificity: 0.8384\n",
      "\n",
      "Time elapsed: -20mins\n",
      "\n",
      "--------------------\n",
      "Training...\n",
      "--------------------\n",
      "Epoch 2:\n",
      "Average training loss: 0.23011409118771553\n",
      "Training accuracy: 0.92\n",
      "\n",
      "\n",
      "Evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  43%|██████████████▌                   | 3/7 [59:37<1:19:23, 1190.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "Average validation loss: 0.27837404515594244\n",
      "Validation Accuracy: 0.8844\n",
      "Validation Precision: 0.8404\n",
      "Validation Recall: 0.9703\n",
      "Validation Specificity: 0.7723\n",
      "\n",
      "Time elapsed: -20mins\n",
      "\n",
      "--------------------\n",
      "Training...\n",
      "--------------------\n",
      "Epoch 3:\n",
      "Average training loss: 0.2044392079114914\n",
      "Training accuracy: 0.912\n",
      "\n",
      "\n",
      "Evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  57%|███████████████████▍              | 4/7 [1:16:41<56:15, 1125.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "Average validation loss: 0.2425926316063851\n",
      "Validation Accuracy: 0.8898\n",
      "Validation Precision: 0.8877\n",
      "Validation Recall: 0.9232\n",
      "Validation Specificity: 0.8406\n",
      "\n",
      "Time elapsed: -17mins\n",
      "\n",
      "--------------------\n",
      "Training...\n",
      "--------------------\n",
      "Epoch 4:\n",
      "Average training loss: 0.15583137911744416\n",
      "Training accuracy: 0.952\n",
      "\n",
      "\n",
      "Evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  71%|████████████████████████▎         | 5/7 [1:35:31<37:33, 1126.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "Average validation loss: 0.2365199935156852\n",
      "Validation Accuracy: 0.8898\n",
      "Validation Precision: 0.8672\n",
      "Validation Recall: 0.9492\n",
      "Validation Specificity: 0.8194\n",
      "\n",
      "Time elapsed: -19mins\n",
      "\n",
      "--------------------\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  71%|████████████████████████▎         | 5/7 [1:48:05<43:14, 1297.16s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">7</span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">4 </span>loss_function = torch.nn.CrossEntropyLoss()                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">5 </span>optimizer = torch.optim.Adam(params=model.parameters(), lr=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">2e-5</span>, eps=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1e-8</span>)                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">6 </span>                                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>7 run_finetuning(model, epochs, training_loader, val_loader, loss_function, optimizer, <span style=\"color: #0000ff; text-decoration-color: #0000ff\">42</span>)     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">8 </span>                                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">run_finetuning</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">61</span>                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 58 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">#print(f\"Accuracy: {num_tr_correct/num_tr_examples}\")</span>                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 59 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">#print('-'*20)</span>                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 60 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>                                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 61 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>loss.backward()                                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 62 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>torch.nn.utils.clip_grad_norm_(model_id.parameters(), <span style=\"color: #0000ff; text-decoration-color: #0000ff\">1.0</span>)                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 63 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>optimizer.step()                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 64 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> warmup:                                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/Users/emilygroves/opt/anaconda3/lib/python3.8/site-packages/torch/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">_tensor.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">487</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">backward</span>    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 484 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>create_graph=create_graph,                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 485 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>inputs=inputs,                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 486 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>)                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 487 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>torch.autograd.backward(                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 488 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, gradient, retain_graph, create_graph, inputs=inputs                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 489 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>)                                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 490 </span>                                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/Users/emilygroves/opt/anaconda3/lib/python3.8/site-packages/torch/autograd/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">__init__.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">200</span> in   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">backward</span>                                                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">197 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># The reason we repeat same the comment below is that</span>                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">198 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># some Python versions print out the first line of a multi-line function</span>               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">199 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># calls in the traceback and some print out the last line</span>                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>200 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>Variable._execution_engine.run_backward(  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Calls into the C++ engine to run the bac</span>   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">201 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>tensors, grad_tensors_, retain_graph, create_graph, inputs,                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">202 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>allow_unreachable=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">True</span>, accumulate_grad=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">True</span>)  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Calls into the C++ engine to ru</span>   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">203 </span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">KeyboardInterrupt</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92m<module>\u001b[0m:\u001b[94m7\u001b[0m                                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m4 \u001b[0mloss_function = torch.nn.CrossEntropyLoss()                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m5 \u001b[0moptimizer = torch.optim.Adam(params=model.parameters(), lr=\u001b[94m2e-5\u001b[0m, eps=\u001b[94m1e-8\u001b[0m)                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m6 \u001b[0m                                                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m7 run_finetuning(model, epochs, training_loader, val_loader, loss_function, optimizer, \u001b[94m42\u001b[0m)     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m8 \u001b[0m                                                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92mrun_finetuning\u001b[0m:\u001b[94m61\u001b[0m                                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 58 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[2m#print(f\"Accuracy: {num_tr_correct/num_tr_examples}\")\u001b[0m                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 59 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[2m#print('-'*20)\u001b[0m                                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 60 \u001b[0m\u001b[2m│   │   │   \u001b[0m                                                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 61 \u001b[2m│   │   │   \u001b[0mloss.backward()                                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 62 \u001b[0m\u001b[2m│   │   │   \u001b[0mtorch.nn.utils.clip_grad_norm_(model_id.parameters(), \u001b[94m1.0\u001b[0m)                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 63 \u001b[0m\u001b[2m│   │   │   \u001b[0moptimizer.step()                                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 64 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mif\u001b[0m warmup:                                                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/Users/emilygroves/opt/anaconda3/lib/python3.8/site-packages/torch/\u001b[0m\u001b[1;33m_tensor.py\u001b[0m:\u001b[94m487\u001b[0m in \u001b[92mbackward\u001b[0m    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 484 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mcreate_graph=create_graph,                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 485 \u001b[0m\u001b[2m│   │   │   │   \u001b[0minputs=inputs,                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 486 \u001b[0m\u001b[2m│   │   │   \u001b[0m)                                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 487 \u001b[2m│   │   \u001b[0mtorch.autograd.backward(                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 488 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[96mself\u001b[0m, gradient, retain_graph, create_graph, inputs=inputs                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 489 \u001b[0m\u001b[2m│   │   \u001b[0m)                                                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 490 \u001b[0m                                                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/Users/emilygroves/opt/anaconda3/lib/python3.8/site-packages/torch/autograd/\u001b[0m\u001b[1;33m__init__.py\u001b[0m:\u001b[94m200\u001b[0m in   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[92mbackward\u001b[0m                                                                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m197 \u001b[0m\u001b[2m│   \u001b[0m\u001b[2m# The reason we repeat same the comment below is that\u001b[0m                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m198 \u001b[0m\u001b[2m│   \u001b[0m\u001b[2m# some Python versions print out the first line of a multi-line function\u001b[0m               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m199 \u001b[0m\u001b[2m│   \u001b[0m\u001b[2m# calls in the traceback and some print out the last line\u001b[0m                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m200 \u001b[2m│   \u001b[0mVariable._execution_engine.run_backward(  \u001b[2m# Calls into the C++ engine to run the bac\u001b[0m   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m201 \u001b[0m\u001b[2m│   │   \u001b[0mtensors, grad_tensors_, retain_graph, create_graph, inputs,                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m202 \u001b[0m\u001b[2m│   │   \u001b[0mallow_unreachable=\u001b[94mTrue\u001b[0m, accumulate_grad=\u001b[94mTrue\u001b[0m)  \u001b[2m# Calls into the C++ engine to ru\u001b[0m   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m203 \u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mKeyboardInterrupt\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run finetuning: Base BERT model, learning rate 2x10-5\n",
    "\n",
    "epochs = 7\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=2e-5, eps=1e-8)\n",
    "\n",
    "run_finetuning(model, epochs, training_loader, val_loader, loss_function, optimizer, 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b910564",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2182d963",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
